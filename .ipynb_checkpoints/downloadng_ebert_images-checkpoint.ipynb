{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e78ad5dd",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/MonashDataFluency/python-web-scraping/blob/master/images/api.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7aac9e",
   "metadata": {},
   "source": [
    "### A brief introduction to APIs\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ff3fd2",
   "metadata": {},
   "source": [
    "Sometimes websites offer an API (or Application Programming Interface) as a service which provides a high level interface to directly retrieve data from their repositories or databases at the backend. \n",
    "\n",
    "From Wikipedia,\n",
    "\n",
    "> \"*An API is typically defined as a set of specifications, such as Hypertext Transfer Protocol (HTTP) request messages, along with a definition of the structure of response messages, usually in an Extensible Markup Language (XML) or JavaScript Object Notation (JSON) format.*\"\n",
    "\n",
    "They typically tend to be URL endpoints (to be fired as requests) that need to be modified based on our requirements (what we desire in the response body) which then returns some a payload (data) within the response, formatted as either JSON, XML or HTML. \n",
    "\n",
    "A popular web architecture style called `REST` (or representational state transfer) allows users to interact with web services via `GET` and `POST` calls (two most commonly used) which we briefly saw in the previous section.\n",
    "\n",
    "For example, Twitter's REST API allows developers to access core Twitter data and the Search API provides methods for developers to interact with Twitter Search and trends data.\n",
    "\n",
    "There are primarily two ways to use APIs :\n",
    "\n",
    "- Through the command terminal using URL endpoints, or\n",
    "- Through programming language specific *wrappers*\n",
    "\n",
    "For example, `Tweepy` is a famous python wrapper for Twitter API whereas `twurl` is a command line interface (CLI) tool but both can achieve the same outcomes.\n",
    "\n",
    "Here we focus on the latter approach and will use a Python library (a wrapper) called `wptools` based around the original MediaWiki API.\n",
    "\n",
    "One advantage of using official APIs is that they are usually compliant of the terms of service (ToS) of a particular service that researchers are looking to gather data from. However, third-party libraries or packages which claim to provide more throughput than the official APIs (rate limits, number of requests/sec) generally operate in a gray area as they tend to violate ToS. Always be sure to read their documentation throughly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b5c0c3",
   "metadata": {},
   "source": [
    "### Wikipedia API\n",
    "\n",
    "Let's say we want to gather some additional data about the Fortune 500 companies and since wikipedia is a rich source for data we decide to use the MediaWiki API to scrape this data. One very good place to start would be to look at the **infoboxes** (as wikipedia defines them) of articles corresponsing to each company on the list. They essentially contain a wealth of metadata about a particular entity the article belongs to which in our case is a company. \n",
    "\n",
    "For e.g. consider the wikipedia article for **Walmart** (https://en.wikipedia.org/wiki/Walmart) which includes the following infobox :\n",
    "\n",
    "![An infobox](https://github.com/MonashDataFluency/python-web-scraping/blob/master/images/infobox.png?raw=1)\n",
    "\n",
    "As we can see from above, the infoboxes could provide us with a lot of valuable information such as :\n",
    "\n",
    "- Year of founding \n",
    "- Industry\n",
    "- Founder(s)\n",
    "- Products\t\n",
    "- Services\t\n",
    "- Operating income\n",
    "- Net income\n",
    "- Total assets\n",
    "- Total equity\n",
    "- Number of employees etc\n",
    "\n",
    "Although we expect this data to be fairly organized, it would require some post-processing which we will tackle in our next section. We pick a subset of our data and focus only on the top **20** of the Fortune 500 from the full list. \n",
    "\n",
    "Let's begin by installing some of libraries we will use for this excercise as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f554fe6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wptools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwptools\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wptools'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 40)\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import imageio as io\n",
    "import matplotlib.pyplot as plt\n",
    "import wptools\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from glob import glob\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import json\n",
    "print('imported!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4669fb",
   "metadata": {},
   "source": [
    "*readng and writing JSON files in python [Stackabuse](http://stackabuse.com/reading-and-writing-json-to-a-file-in-python/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3346a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create the bestofrt_posters f it doesn;t exist\n",
    "\n",
    "folder_name = './datasets/bestofrt_posters'\n",
    "\n",
    "if not os.path.exists(folder_name):\n",
    "    os.mkdir(folder_name)\n",
    "    print('Folder created!')\n",
    "else:\n",
    "    print('folder exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c681e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.listdir('datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdee2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = [\n",
    " 'The_Wizard_of_Oz_(1939_film)',\n",
    " 'Citizen_Kane',\n",
    " 'The_Third_Man',\n",
    " 'Get_Out_(film)',\n",
    " 'Mad_Max:_Fury_Road',\n",
    " 'The_Cabinet_of_Dr._Caligari',\n",
    " 'All_About_Eve',\n",
    " 'Inside_Out_(2015_film)',\n",
    " 'The_Godfather',\n",
    " 'Metropolis_(1927_film)',\n",
    " 'E.T._the_Extra-Terrestrial',\n",
    " 'Modern_Times_(film)',\n",
    " 'It_Happened_One_Night',\n",
    " \"Singin'_in_the_Rain\",\n",
    " 'Boyhood_(film)',\n",
    " 'Casablanca_(film)',\n",
    " 'Moonlight_(2016_film)',\n",
    " 'Psycho_(1960_film)',\n",
    " 'Laura_(1944_film)',\n",
    " 'Nosferatu',\n",
    " 'Snow_White_and_the_Seven_Dwarfs_(1937_film)',\n",
    " \"A_Hard_Day%27s_Night_(film)\",\n",
    " \"A_Hard_Day's_Night\",\n",
    " 'La_Grande_Illusion',\n",
    " 'North_by_Northwest',\n",
    " 'The_Battle_of_Algiers',\n",
    " 'Dunkirk_(2017_film)',\n",
    " 'The_Maltese_Falcon_(1941_film)',\n",
    " 'Repulsion_(film)',\n",
    " '12_Years_a_Slave_(film)',\n",
    " 'Gravity_(2013_film)',\n",
    " 'Sunset_Boulevard_(film)',\n",
    " 'King_Kong_(1933_film)',\n",
    " 'Spotlight_(film)',\n",
    " 'The_Adventures_of_Robin_Hood',\n",
    " 'Rashomon',\n",
    " 'Rear_Window',\n",
    " 'Selma_(film)',\n",
    " 'Taxi_Driver',\n",
    " 'Toy_Story_3',\n",
    " 'Argo_(2012_film)',\n",
    " 'Toy_Story_2',\n",
    " 'The_Big_Sick',\n",
    " 'Bride_of_Frankenstein',\n",
    " 'Zootopia',\n",
    " 'M_(1931_film)',\n",
    " 'Wonder_Woman_(2017_film)',\n",
    " 'The_Philadelphia_Story_(film)',\n",
    " 'Alien_(film)',\n",
    " 'Bicycle_Thieves',\n",
    " 'Seven_Samurai',\n",
    " 'The_Treasure_of_the_Sierra_Madre_(film)',\n",
    " 'Up_(2009_film)',\n",
    " '12_Angry_Men_(1957_film)',\n",
    " 'The_400_Blows',\n",
    " 'Logan_(film)',\n",
    " 'All_Quiet_on_the_Western_Front_(1930_film)',\n",
    " 'Army_of_Shadows',\n",
    " 'Arrival_(film)',\n",
    " 'Baby_Driver',\n",
    " 'A_Streetcar_Named_Desire_(1951_film)',\n",
    " 'The_Night_of_the_Hunter_(film)',\n",
    " 'Star_Wars:_The_Force_Awakens',\n",
    " 'Manchester_by_the_Sea_(film)',\n",
    " 'Dr._Strangelove',\n",
    " 'Frankenstein_(1931_film)',\n",
    " 'Vertigo_(film)',\n",
    " 'The_Dark_Knight_(film)',\n",
    " 'Touch_of_Evil',\n",
    " 'The_Babadook',\n",
    " 'The_Conformist_(film)',\n",
    " 'Rebecca_(1940_film)',\n",
    " \"Rosemary%27s_Baby_(film)\",\n",
    " 'Finding_Nemo',\n",
    " 'Brooklyn_(film)',\n",
    " 'The_Wrestler_(2008_film)',\n",
    " 'The_39_Steps_(1935_film)',\n",
    " 'L.A._Confidential_(film)',\n",
    " 'Gone_with_the_Wind_(film)',\n",
    " 'The_Good,_the_Bad_and_the_Ugly',\n",
    " 'Skyfall',\n",
    " 'Rome,_Open_City',\n",
    " 'Tokyo_Story',\n",
    " 'Hell_or_High_Water_(film)',\n",
    " 'Pinocchio_(1940_film)',\n",
    " 'The_Jungle_Book_(2016_film)',\n",
    " 'La_La_Land_(film)',\n",
    " 'Star_Trek_(film)',\n",
    " 'High_Noon',\n",
    " 'Apocalypse_Now',\n",
    " 'On_the_Waterfront',\n",
    " 'The_Wages_of_Fear',\n",
    " 'The_Last_Picture_Show',\n",
    " 'Harry_Potter_and_the_Deathly_Hallows_â€“_Part_2',\n",
    " 'The_Grapes_of_Wrath_(film)',\n",
    " 'Roman_Holiday',\n",
    " 'Man_on_Wire',\n",
    " 'Jaws_(film)',\n",
    " 'Toy_Story',\n",
    " 'The_Godfather_Part_II',\n",
    " 'Battleship_Potemkin'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a9dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dictionaries to build and convert to a DataFrame later\n",
    "df_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e25622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_poster_url(title):\n",
    "    page = wptools.page(title, silent=True).get()\n",
    "    imgs = page.data['image']\n",
    "    url = imgs[0]['url']\n",
    "    return imgs, url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a23b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_imgs(first_image_url):\n",
    "    # Download movie poster image using imageio\n",
    "    # from HTTPs\n",
    "    frames = io.imread(first_image_url)\n",
    "    image_file_format = first_image_url.split('.')[-1]\n",
    "    # save the image\n",
    "    img_name = folder_name + \"/\" + str(ranking) + \"_\" + title + '.' + image_file_format\n",
    "    matplotlib.image.imsave(img_name, frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18a4732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_img(first_image_url):\n",
    "    # Download movie poster image PIL\n",
    "    r = requests.get(first_image_url)\n",
    "    i = Image.open(BytesIO(r.content))\n",
    "    image_file_format = first_image_url.split('.')[-1]\n",
    "    i.save(folder_name + \"/\" + str(ranking) + \"_\" + title + '.' + image_file_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe80a8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dctionary for holdng images that had issues downloading\n",
    "\n",
    "image_errors = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6bcc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for title in title_list:\n",
    "        try:\n",
    "            # This cell is slow so print ranking to gauge time remaining\n",
    "            ranking = title_list.index(title) + 1\n",
    "            print(ranking)\n",
    "\n",
    "            # First image is usually the poster\n",
    "            images, first_image_url = get_poster_url(title)\n",
    "\n",
    "            # Download images\n",
    "            download_imgs(first_image_url)\n",
    "\n",
    "            # Append to list of dictionaries\n",
    "            df_list.append({'ranking': int(ranking),\n",
    "                            'title': title,\n",
    "                            'poster_url': first_image_url})\n",
    "\n",
    "        # Not best practice to catch all exceptions but fine for this short script\n",
    "        except Exception as e:\n",
    "            print(str(ranking) + \"_\" + title + \": \" + str(e))\n",
    "            image_errors[str(ranking) + \"_\" + title] = images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace1499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for img in image_errors.keys():\n",
    "    print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70106940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the error dict as a json object\n",
    "\n",
    "with open('img_errors.json', 'w') as fp:\n",
    "    json.dump(image_errors, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecca9911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from list of dictionaries\n",
    "df = pd.DataFrame(df_list, columns = ['ranking', 'title', 'poster_url'])\n",
    "df = df.sort_values('ranking').reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eb09a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ebert_imgs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
