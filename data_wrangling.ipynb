{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "981b91c4",
   "metadata": {},
   "source": [
    "## Gather Data: [Download](https://www.kaggle.com/udacity/armenian-online-job-postings)\n",
    "The dataset used in this lesson is hosted on this Kaggle Datasets page: Armenian Online Job Postings. Some context on this dataset, from the description section of that page:\n",
    "\n",
    "The online job market is a good indicator of overall demand for labor in an economy. This dataset consists of 19,000 job postings from 2004 to 2015 posted on CareerCenter, an Armenian human resource portal.\n",
    "\n",
    "Since postings are text documents and tend to have similar structures, text mining can be used to extract features like posting date, job title, company name, job description, salary, and more. Postings that had no structure or were not job-related were removed. The data was originally scraped from a Yahoo! mailing group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba63008",
   "metadata": {},
   "source": [
    "Two Reasons Why downloading data is better with code than manually are:\n",
    "1. **Reproducibility**\tThe ability of a process to produce the same results from identical inputs\n",
    "2. **Scalability**\tThe ability of a process to handle an increasing scope of work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b893b8",
   "metadata": {},
   "source": [
    "`<img src='./img/example-job-posting.jpg' width=400 height=200/>`\n",
    "```\n",
    "with open('./datasets/features.txt', 'r') as f:\n",
    "    pprint.pprint(f.readlines())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "382b2405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Imported\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',None)\n",
    "#pd.set_option('display.max_rows',None)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "print('All Imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acc4005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move any files\n",
    "def move_file_types(source, dst, pattern):\n",
    "    files = []\n",
    "    for type_ in pattern:\n",
    "        files.extend(glob.glob(f'{source}{type_}'))\n",
    "        \n",
    "    if files:\n",
    "        for file in files:\n",
    "            shutil.move(f'{file}', f'{dst}')\n",
    "            \n",
    "    print(f'{len(files)} file(s) moved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10eaee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.idea',\n",
       " '.ipynb_checkpoints',\n",
       " 'ALX-T Connect Weekly Schedule - Data.pdf',\n",
       " 'certs',\n",
       " 'cleaning_and_splitting_movie_genres.ipynb',\n",
       " 'connect_sessions_studyguide.pdf',\n",
       " 'data-wrangling-template.ipynb',\n",
       " 'datasets',\n",
       " 'dataviz_concepts.ipynb',\n",
       " 'data_tips.ipynb',\n",
       " 'data_visualization.ipynb',\n",
       " 'data_wrangling.ipynb',\n",
       " 'downloadng_ebert_images.ipynb',\n",
       " 'ebert_imgs.csv',\n",
       " 'extract_attendance_from_cloudSQL.ipynb',\n",
       " 'figs_axes_subplots.ipynb',\n",
       " 'img',\n",
       " 'img_errors.json',\n",
       " 'intro_to_functions.ipynb',\n",
       " 'lesson3_the_data_analysis_process.ipynb',\n",
       " 'load_students_table_cloudsql.ipynb',\n",
       " 'media_wiki.ipynb',\n",
       " 'merging-data.ipynb',\n",
       " 'merry.txt',\n",
       " 'multivariate_plots.ipynb',\n",
       " 'name.png',\n",
       " 'regex_python.ipynb',\n",
       " 'regex_python_googleDevs.ipynb',\n",
       " 'testing.ipynb',\n",
       " 'udacity project 1.zip',\n",
       " 'univariate-data-viz.ipynb',\n",
       " 'web_scraping.ipynb',\n",
       " 'wine_visualizations_solutions.ipynb']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a749e47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source = 'datasets/'\n",
    "dst = 'img/'\n",
    "types = ('*.jpg', '*.png')\n",
    "\n",
    "# move any \n",
    "try:\n",
    "    move_file_types(source, dst, pattern=types)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5fd136",
   "metadata": {},
   "source": [
    "*zipfile is also a context manager, therefore supports the `with` statement*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065ddfea",
   "metadata": {},
   "source": [
    "### Context Manager:\n",
    "\n",
    "A protocol for handling resources in Python, e.g. the with statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9f801f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uto's file extracted\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile(\"udacity project 1.zip\", 'r') as myzip:\n",
    "    myzip.extractall(\"./datasets/uto/\")\n",
    "print('Uto\\'s file extracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0dd906",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"datasets/archive.zip\", 'r') as myzip:\n",
    "    myzip.extractall(\"./datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317c2255",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.listdir('datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f83945c",
   "metadata": {},
   "source": [
    "*move the image file to the image dirctory*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c36146",
   "metadata": {},
   "source": [
    "### What is CSV?\n",
    "\n",
    "CSV stands for comma separated values, and is a common file format to represent tabular data.\n",
    "\n",
    "Can be created and edited using any text editor\n",
    "Often created by exporting data from a spreadsheet or database. On the right here, we have the CSV file opened in a\n",
    "CSV files use commas as separators, also known as delimiters.\n",
    "\n",
    "Each line of data represents a row. The delimiters (commas) separate the data in each row into columns.\n",
    "\n",
    "There are many other file formats including JSON, txt, and html. We'll cover some of those in the next lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c4e08f",
   "metadata": {},
   "source": [
    "### EDA ( Exploratory Data Analysis)\n",
    "\n",
    "Here is one definition of EDA: an analysis approach that focuses on identifying general patterns in the data, and identifying outliers and features of the data that might not have been anticipated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b16723b",
   "metadata": {},
   "source": [
    "### Data wrangling \n",
    "is about gathering the right pieces of data, assessing your data's quality and structure, then modifying your data to make it clean. But the assessments you make and convert to cleaning operations won't make your analysis, visualization, or model better, though. The goal is to just make them possible, i.e., functional.\n",
    "\n",
    "### EDA \n",
    "is about exploring your data to later augment it to maximize the potential of our analyses, visualizations, and models. When exploring, simple visualizations are often used to summarize your data's main characteristics. From there you can do things like remove outliers and create new and more descriptive features from existing data, also known as feature engineering. Or detect and remove outliers so your model's fit is better.\n",
    "\n",
    "In practice, wrangling and EDA can and often do occur together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6268fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df = pd.read_csv('datasets/online-job-postings.csv')\n",
    "jobs_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4d8696",
   "metadata": {},
   "source": [
    "![Job Postings](./img/example-job-posting.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d7d8df",
   "metadata": {},
   "source": [
    "## Data Quality\n",
    "Low quality data is commonly referred to as dirty data. Dirty data has issues with its content.\n",
    "\n",
    "* missing data, \n",
    "* invalid data, \n",
    "* inaccurate date or \n",
    "* inconsistent data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce3ba24",
   "metadata": {},
   "source": [
    "## Tidiness\n",
    "Untidy data is commonly referred to as \"messy\" data. Messy data has issues with its structure.\n",
    "\n",
    "A dataset is messy or tidy depending on how **rows, columns, and tables** are matched up with **observations, variables, and types**. In tidy data:\n",
    "\n",
    "* Each variable forms a column.\n",
    "* Each observation forms a row.\n",
    "* Each type of observational unit forms a table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e715c",
   "metadata": {},
   "source": [
    "New Terms\n",
    "Term\tDefinition\n",
    "\n",
    "* **Dirty Data**:\tData that has issues with data content including missing data, invalid data, inaccurate date or inconsistent data\n",
    "* **Messy Data**:\tData that has issues with its structure (columns, rows or table)\n",
    "* **Programmatic Assessment**:\tReviewing data using code\n",
    "* **Visual Assessment**:\tReviewing data by scrolling through it in a spreadsheet or text editing application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe420dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee328a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df9931b",
   "metadata": {},
   "source": [
    "* Missing values **(NaN)**\n",
    "* StartDate inconsistencies **(ASAP)**\n",
    "* Fix non-descriptive column headers\n",
    "* Maybe make columns all lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62688350",
   "metadata": {},
   "source": [
    "### What is Data Cleaning?\n",
    "Cleaning means acting on the assessments we made to improve quality and tidiness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1b5c49",
   "metadata": {},
   "source": [
    "#### The Programmatic Data Cleaning Process\n",
    "1. Define\n",
    "2. Code\n",
    "3. Test\n",
    "\n",
    "* Defining means defining a data cleaning plan in writing, where we turn our assessments into defined cleaning tasks. This plan will also serve as an instruction list so others (or us in the future) can look at our work and reproduce it.\n",
    "\n",
    "* Coding means translating these definitions to code and executing that code.\n",
    "\n",
    "* Testing means testing our dataset, often using code, to make sure our cleaning operations worked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b4aaa1",
   "metadata": {},
   "source": [
    "**Defining What We Want to Clean:**\n",
    "\n",
    "Defining what we want to clean is creating a cleaning plan or instruction list where we convert the notes we made in the Assess step into specific cleaning tasks.\n",
    "\n",
    "We identified three issues\n",
    "\n",
    "* Missing values or NaNs\n",
    "* Start date inconsistencies for ASAP\n",
    "* Non-descriptive column headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bbaabd",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    "In the Jupyter Notebook below, in your own words, convert the assessments we made previously into defined cleaning operations, as shown in the above video. The missing values (NaN) and untidy dataset issues will not be cleaned in this lesson, so do not write a definition for those assessments.\n",
    "\n",
    "* Write a cleaning dfinition for the startdates inconsistencies (ASAP)\n",
    "* Write a cleaning definition for the nin-descriptive column headers\n",
    "\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "* Select all nondescriptive and misspelled column headers (ApplicationP, AboutC, RequiredQual, JobRequirment) and replace them with full words (ApplicationProcedure, AboutCompany, RequiredQualifications, JobRequirement)\n",
    "* Select all records in the StartDate column that have \"As soon as possible\", \"Immediately\", etc. and replace the text in those cells with \"ASAP\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea92773",
   "metadata": {},
   "source": [
    "### Clean\n",
    "#### Types of cleaning:\n",
    "* Manual (not recommended unless the issues are single occurrences)\n",
    "* Programmatic\n",
    "\n",
    "**The programmatic data cleaning process:**\n",
    "\n",
    "1. Define: convert our assessments into defined cleaning tasks. These definitions also serve as an instruction list so others (or yourself in the future) can look at your work and reproduce it.\n",
    "2. Code: convert those definitions to code and run that code.\n",
    "3. Test: test your dataset, visually or with code, to make sure your cleaning operations worked.\n",
    "Always make copies of the original pieces of data before cleaning!\n",
    "\n",
    "### Reassess and Iterate\n",
    "After cleaning, always reassess and iterate on any of the data wrangling steps if necessary.\n",
    "### Store (Optional)\n",
    "Store data, in a file or database for example, if you need to use it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b753d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = jobs_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5765995",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.rename(columns={'ApplicationP':'ApplicationProcedure',\n",
    "                        'AboutC':'AboutCompany',\n",
    "                        'RequiredQual':'RequiredQualifications',\n",
    "                        'JobRequirment':'JobRequirements'},\n",
    "               inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1686b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "asap_list = ['Immediately', 'As soon as possible', 'Upon hiring',\n",
    "             'Immediate', 'Immediate employment', 'As soon as possible.', 'Immediate job opportunity',\n",
    "             '\"Immediate employment, after passing the interview.\"',\n",
    "             'ASAP preferred', 'Employment contract signature date',\n",
    "             'Immediate employment opportunity', 'Immidiately', 'ASA',\n",
    "             'Asap', '\"The position is open immediately but has a flexible start date depending on the candidates earliest availability.\"',\n",
    "             'Immediately upon agreement', '20 November 2014 or ASAP',\n",
    "             'immediately', 'Immediatelly',\n",
    "             '\"Immediately upon selection or no later than November 15, 2009.\"',\n",
    "             'Immediate job opening', 'Immediate hiring', 'Upon selection',\n",
    "             'As soon as practical', 'Immadiate', 'As soon as posible',\n",
    "             'Immediately with 2 months probation period',\n",
    "             '12 November 2012 or ASAP', 'Immediate employment after passing the interview',\n",
    "             'Immediately/ upon agreement', '01 September 2014 or ASAP',\n",
    "             'Immediately or as per agreement', 'as soon as possible',\n",
    "             'As soon as Possible', 'in the nearest future', 'immediate',\n",
    "             '01 April 2014 or ASAP', 'Immidiatly', 'Urgent',\n",
    "             'Immediate or earliest possible', 'Immediate hire',\n",
    "             'Earliest  possible', 'ASAP with 3 months probation period.',\n",
    "             'Immediate employment opportunity.', 'Immediate employment.',\n",
    "             'Immidietly', 'Imminent', 'September 2014 or ASAP', 'Imediately']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb049b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the df.col.replace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb7635f",
   "metadata": {},
   "source": [
    "[BeautifulSoup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find)\n",
    "\n",
    "[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8090fe5",
   "metadata": {},
   "source": [
    "### Quiz\n",
    "\n",
    "With your knowledge of HTML file structure, you're going to use Beautiful Soup to extract our desired Audience Score metric and number of audience ratings, along with the movie title like in the video above (so we have something to merge the datasets on later) for each HTML file, then save them in a pandas DataFrame.\n",
    "\n",
    "The Jupyter Notebook below contains template code that:\n",
    "\n",
    "* Creates an empty list, `df_list`, to which dictionaries will be appended. \n",
    "* This list of dictionaries will eventually be converted to a pandas DataFrame (this is the most efficient way of building a DataFrame row by row).\n",
    "* Loops through each movie's Rotten Tomatoes HTML file in the rt_html folder.\n",
    "* Opens each HTML file and passes it into a filehandle called file.\n",
    "* Creates a DataFrame called df by converting df_list using the pd.DataFrame constructor.\n",
    "* Your task is to extract the title, audience score, and the number of audience ratings in each HTML file so each trio can be appended as a dictionary to df_list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadb7dc2",
   "metadata": {},
   "source": [
    "1. \n",
    "```\n",
    "<div class=\"meter-value\">\n",
    "      <span class=\"superPageFontColor\" style=\"vertical-align:top\">97%</span>\n",
    "</div>\n",
    "```\n",
    "\n",
    "2. \n",
    "```\n",
    "<div class=\"audience-info hidden-xs superPageFontColor\">\n",
    "    <div>\n",
    "            <span class=\"subtle superPageFontColor\">Average Rating:</span>\n",
    "            3.5/5\n",
    "                </div>\n",
    "    <div>\n",
    "        <span class=\"subtle superPageFontColor\">User Ratings:</span>\n",
    "        32,313,030</div>\n",
    "</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a0be71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
